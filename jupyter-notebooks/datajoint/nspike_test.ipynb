{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start mysql docker: sudo docker-compose up -d\n",
    "\n",
    "import datajoint as dj\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "import traceback\n",
    "import glob\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorruptData(Exception):\n",
    "    pass\n",
    "\n",
    "class InconsistentData(Warning):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.abspath(os.path.join(globals()['_dh'][0], '../..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_cred_filename = os.path.join(root_path, 'datajoint/local_cred.ini')\n",
    "with open(local_cred_filename) as f:\n",
    "    local_cred = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred_filename = \"~/\"\n",
    "\n",
    "dj.config['database.host'] = local_cred['host']\n",
    "dj.config['database.user'] = local_cred['user']\n",
    "dj.config['database.password'] = local_cred['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.conn().cancel_transaction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = dj.schema('franklab_nspike', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = dj.schema('franklab_nspike', locals())\n",
    "schema.drop()\n",
    "schema = dj.schema('franklab_nspike', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RippleDetectionConfig().drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WarningTracker:\n",
    "    def __init__(self, suppress_print=False):\n",
    "        self.suppress_print = suppress_print\n",
    "        self.missing_entries = []\n",
    "        self.duplicate_entries = []\n",
    "        self.duplicate_nonmatch_entries = []\n",
    "        \n",
    "    def missing_field(self, key, field):\n",
    "        if(not self.suppress_print):\n",
    "             warnings.warn('Missing {:s}: {:s}'.format(str(key), field), InconsistentData)\n",
    "        self.missing_entries.append((key, field))\n",
    "    \n",
    "    def duplicate(self, key, field, using_first=True):\n",
    "        using_first = ''\n",
    "        if using_first:\n",
    "            first_str = ' using first'\n",
    "        if(not self.suppress_print):\n",
    "            warnings.warn('Duplicate entry for {:s}: {:s}{:s}.'.\n",
    "                          format(str(key), field, first_str), InconsistentData)\n",
    "        self.duplicate_nonmatch_entries.append((key, field, value1, value2))        \n",
    "    \n",
    "    def duplicate_nonmatch(self, key, field, value1, value2):\n",
    "        if(not self.suppress_print):\n",
    "            warnings.warn('Duplicate entry for {:s}: {:s}, values are({:s}, {:s})'.\n",
    "                          format(str(key), field, value1, value2), InconsistentData)\n",
    "        self.duplicate_nonmatch_entries.append((key, field, value1, value2))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ('missing_entries:\\n' + '\\n'.join(map(str, self.missing_entries)) + '\\n' +\n",
    "                'duplicate_entries:\\n' + '\\n'.join(map(str, self.duplicate_entries)) + '\\n' +\n",
    "                'duplicate_nonmatch_entries:\\n' + '\\n'.join(map(str, self.duplicate_nonmatch_entries)))\n",
    "    \n",
    "\n",
    "@schema\n",
    "class Animal(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    anim_name: varchar(20)  #Name of animal\n",
    "    ---\n",
    "    anim_name_short: varchar(10)\n",
    "    anim_path_raw: varchar(200)\n",
    "    anim_path_mat: varchar(200)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "\n",
    "\n",
    "@schema\n",
    "class Day(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Animal\n",
    "    day: int\n",
    "    ---\n",
    "    day_path_raw: varchar(200)\n",
    "    day_path_mat: varchar(200)\n",
    "    day_start_time_sec: float\n",
    "    day_end_time_sec: float\n",
    "    day_start_time_nspike: int\n",
    "    day_end_time_nspike: int\n",
    "    \"\"\"\n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "        \n",
    "    def make(self, key):\n",
    "        \n",
    "        anim_name, anim_path_raw, anim_path_mat = (Animal() & key).fetch1('anim_name', 'anim_path_raw', 'anim_path_mat')\n",
    "        dir_names = os.listdir(anim_path_raw)\n",
    "        for dir_name in dir_names:\n",
    "            m = re.search('^{:s}(\\d*)$'.format(anim_name.lower()), dir_name)\n",
    "            if m:\n",
    "                day = int(m.groups()[0])\n",
    "                day_path_raw = os.path.join(anim_path_raw, dir_name)\n",
    "                times_path = os.path.join(day_path_raw, 'times.mat')\n",
    "                if os.path.isfile(times_path):\n",
    "                    times_mat = loadmat(times_path)\n",
    "                    time_ranges = times_mat['ranges']\n",
    "                    day_start_time_nspike = time_ranges[0][0]\n",
    "                    day_end_time_nspike = time_ranges[0][1]\n",
    "                    day_start_time_sec = day_start_time_nspike/10000\n",
    "                    day_end_time_sec = day_end_time_nspike/10000\n",
    "                    self.insert1({'anim_name': anim_name,\n",
    "                                  'day': day,\n",
    "                                  'day_path_raw': day_path_raw,\n",
    "                                  'day_path_mat': anim_path_mat,\n",
    "                                  'day_start_time_sec': day_start_time_nspike,\n",
    "                                  'day_end_time_sec': day_end_time_nspike,\n",
    "                                  'day_start_time_nspike': day_start_time_sec,\n",
    "                                  'day_end_time_nspike': day_end_time_sec})\n",
    "                else:\n",
    "                    # Missing times.mat means data folder was not processed for spike sorting (matclust)\n",
    "                    self.warn.missing_field(key, times_path)\n",
    "                    pass\n",
    "\n",
    "                \n",
    "@schema\n",
    "class Epoch(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Day\n",
    "    epoch_id: tinyint\n",
    "    ---\n",
    "    epoch_name: varchar(50)\n",
    "    epoch_time_str: varchar(50)\n",
    "    epoch_start_time_sec: float\n",
    "    epoch_end_time_sec: float\n",
    "    epoch_start_time_nspike: int\n",
    "    epoch_end_time_nspike: int\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "        \n",
    "    def make(self, key):\n",
    "        anim_name, day, day_path_raw = (Animal() * (Day() & key)).fetch1('anim_name', 'day', 'day_path_raw')\n",
    "        try:\n",
    "            times_mat = loadmat(os.path.join(day_path_raw, 'times.mat'))\n",
    "            time_ranges = times_mat['ranges']\n",
    "            names = times_mat['names']\n",
    "            for epoch_id, epoch_time_range in enumerate(time_ranges[1:]):\n",
    "                epoch_start_time_nspike = epoch_time_range[0]\n",
    "                epoch_end_time_nspike = epoch_time_range[1]\n",
    "                epoch_start_time_sec = epoch_start_time_nspike/10000\n",
    "                epoch_end_time_sec = epoch_end_time_nspike/10000\n",
    "                name_entry = names[epoch_id + 1][0][0]\n",
    "                name_re = re.search('^\\d*\\s*(\\w*)\\s*([0-9:\\-_]*)$', name_entry)\n",
    "                if name_re:\n",
    "                    epoch_name = name_re.groups()[0]\n",
    "                    epoch_time_str = name_re.groups()[1]\n",
    "                    self.insert1({'anim_name': anim_name,\n",
    "                                  'day': day, \n",
    "                                  'epoch_id': epoch_id, \n",
    "                                  'epoch_name': epoch_name,\n",
    "                                  'epoch_time_str': epoch_time_str,\n",
    "                                  'epoch_start_time_sec': epoch_start_time_sec,\n",
    "                                  'epoch_end_time_sec':epoch_start_time_sec,\n",
    "                                  'epoch_start_time_nspike': epoch_start_time_nspike,\n",
    "                                  'epoch_end_time_nspike': epoch_end_time_nspike\n",
    "                                 })\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self.warn.missing_field(key, times_path)\n",
    "    \n",
    "@schema\n",
    "class Tetrode(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Animal\n",
    "    tet_id: tinyint\n",
    "    ---\n",
    "    tet_hemisphere: varchar(50)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "        \n",
    "    def make(self, key):\n",
    "        anim_name, anim_path_mat = (Animal() & key).fetch1('anim_name', 'anim_path_mat')\n",
    "        mat = loadmat(os.path.join(anim_path_mat, 'bontetinfo.mat'))\n",
    "        tet = {}\n",
    "        for tet_days in mat['tetinfo'][0]:\n",
    "            if len(tet_days[0]) > 0:\n",
    "                for tet_epochs in tet_days[0]:\n",
    "                    for tet_id, tet_epoch in enumerate(tet_epochs[0]):\n",
    "                        if len(tet_epoch[0]) > 0:\n",
    "                            tet_entry = tet.setdefault(tet_id, {})\n",
    "                            tet_entry_hemi_list = tet_entry.setdefault('hemisphere', [])\n",
    "                            try:\n",
    "                                tet_hemi = tet_epoch[0][0]['hemisphere'][0]\n",
    "                            except ValueError:\n",
    "                                tet_hemi = None\n",
    "                            tet_entry_hemi_list.append(tet_hemi)\n",
    "\n",
    "        for tet_id, tet_entries in tet.items():\n",
    "            tet_hemi = tet_entries['hemisphere']\n",
    "            tet_hemi_set = set(tet_hemi)\n",
    "            if len(tet_hemi_set) == 1:\n",
    "                tet_hemisphere = list(tet_hemi_set)[0]\n",
    "                if tet_hemisphere is None:\n",
    "                    tet_hemisphere = ''\n",
    "                self.insert1({'anim_name': anim_name,\n",
    "                              'tet_id': tet_id,\n",
    "                              'tet_hemisphere': tet_hemisphere\n",
    "                             })\n",
    "            else:\n",
    "                warn.duplicate_nonmatch(key, 'hemisphere', tet, list(tet_hemi_set)[0], list(tet_hemi_set)[1:])\n",
    "\n",
    "\n",
    "@schema\n",
    "class TetrodeEpoch(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    -> Tetrode\n",
    "    -> Epoch\n",
    "    ---\n",
    "    tet_depth = NULL: int\n",
    "    tet_num_cells = NULL: int\n",
    "    tet_area = NULL: varchar(50)\n",
    "    tet_subarea = NULL: varchar(50)\n",
    "    tet_near_ca2 = NULL: tinyint       # boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "    \n",
    "    def make(self, key):\n",
    "        anim_name, anim_path_mat = (Animal() & key).fetch1('anim_name', 'anim_path_mat')\n",
    "        try:\n",
    "            mat = self.anim_tet_infos[anim_name]\n",
    "        except AttributeError:\n",
    "            self.anim_tet_infos = {}\n",
    "            mat = loadmat(os.path.join(anim_path_mat, 'bontetinfo.mat'))\n",
    "            self.anim_tet_infos[anim_name] = mat\n",
    "        except KeyError:\n",
    "            mat = loadmat(os.path.join(anim_path_mat, 'bontetinfo.mat'))\n",
    "            self.anim_tet_infos[anim_name] = mat            \n",
    "            \n",
    "        try:\n",
    "            tet_epoch_data = mat['tetinfo'][0][key['day']-1][0][key['epoch_id']][0][key['tet_id']][0]\n",
    "\n",
    "            # if mat cell is empty, skip insert\n",
    "            if tet_epoch_data.size > 0:\n",
    "                try:\n",
    "                    key['tet_depth'] = tet_epoch_data['depth'][0][0][0][0][0]\n",
    "                except (ValueError, IndexError):\n",
    "                    self.warn.missing_field(key, 'tet_depth')\n",
    "                    # print(key)\n",
    "                    # leave entry out\n",
    "                    pass\n",
    "                    # key['tet_depth'] = None\n",
    "                try:\n",
    "                    key['tet_num_cells'] = tet_epoch_data['numcells'][0][0][0]\n",
    "                except (ValueError, IndexError):\n",
    "                    self.warn.missing_field(key, 'tet_num_cells')\n",
    "                    # print(key)\n",
    "                    # leave entry out\n",
    "                    pass\n",
    "                    # key['tet_num_cells'] = None\n",
    "                try:\n",
    "                    key['tet_area'] = tet_epoch_data['area'][0][0]\n",
    "                except (ValueError, IndexError):\n",
    "                    self.warn.missing_field(key, 'tet_area')\n",
    "                    # print(key)\n",
    "                    # leave entry out\n",
    "                    pass\n",
    "                    # key['tet_area'] = None\n",
    "                try:\n",
    "                    key['tet_subarea'] = tet_epoch_data['subarea'][0][0]\n",
    "                except (ValueError, IndexError):\n",
    "                    self.warn.missing_field(key, 'tet_subarea')\n",
    "                    # print(key)\n",
    "                    # leave entry out\n",
    "                    pass\n",
    "                    # key['tet_subarea'] = None\n",
    "                try:\n",
    "                    key['tet_near_ca2'] = tet_epoch_data['nearCA2'][0][0][0]\n",
    "                except (ValueError, IndexError):\n",
    "                    self.warn.missing_field(key, 'tet_near_ca2')\n",
    "                    # print(key)\n",
    "                    # leave entry out\n",
    "                    pass\n",
    "                    # key['tet_near_ca2'] = None\n",
    "                self.insert1(key)\n",
    "        except (ValueError, IndexError):\n",
    "            self.warn.missing_field(key, 'entire tetrode')\n",
    "            # print(key)\n",
    "            pass\n",
    "\n",
    "@schema\n",
    "class LFP(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> TetrodeEpoch\n",
    "    ---\n",
    "    lfp_filepath_eeg_mat = NULL: varchar(200)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "\n",
    "    def make(self, key):\n",
    "        day_path_mat = (Day() & key).fetch1('day_path_mat')\n",
    "        try:\n",
    "            lfp_filepath_eeg_mats = glob.glob(os.path.join(day_path_mat, 'EEG/boneeg{:02d}-{:d}-{:02d}.mat'.\n",
    "                                                           format(key['day'], key['epoch_id']+1, key['tet_id']+1)))\n",
    "            \n",
    "            if len(lfp_filepath_eeg_mats) > 1:\n",
    "                self.warn.duplicate(key, lfp_filepath_eeg_mats, using_first=True)\n",
    "            lfp_filepath_eeg_mat = lfp_filepath_eeg_mats[0]\n",
    "            \n",
    "            key['lfp_filepath_eeg_mat'] = lfp_filepath_eeg_mat\n",
    "        except IndexError:\n",
    "            self.warn.missing_field(key, 'eeg mat')\n",
    "\n",
    "        self.insert1(key)\n",
    "\n",
    "\n",
    "@schema\n",
    "class LFPRaw(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> TetrodeEpoch\n",
    "    ---\n",
    "    lfp_filepath_raw = NULL: varchar(200)\n",
    "    lfp_tet_depth = NULL: int\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "\n",
    "    def make(self, key):\n",
    "        day_path_raw, day_path_mat = (Day() & key).fetch1('day_path_raw', 'day_path_mat')\n",
    "        tet_id = key['tet_id']\n",
    "        lfp_filepath_raw = glob.glob(os.path.join(day_path_raw, '{:02d}-*.eeg').format(tet_id+1))[0]\n",
    "        lfp_filename_raw = os.path.basename(lfp_filepath_raw)\n",
    "        re_match = re.search('\\d*-(\\d*).eeg$', lfp_filename_raw)\n",
    "        lfp_tet_depth_str = re_match.groups()[0]\n",
    "        lfp_tet_depth = int(lfp_tet_depth_str)\n",
    "\n",
    "        key['lfp_filepath_raw'] = lfp_filepath_raw\n",
    "        key['lfp_tet_depth'] = lfp_tet_depth\n",
    "\n",
    "        self.insert1(key)\n",
    "     \n",
    "    \n",
    "@schema\n",
    "class LFPGnd(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> TetrodeEpoch\n",
    "    ---\n",
    "    lfp_filepath_eeggnd_mat = NULL: varchar(200)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "\n",
    "    def make(self, key):\n",
    "        day_path_mat = (Day() & key).fetch1('day_path_mat')\n",
    "        \n",
    "        try:\n",
    "            lfp_filepath_eeggnd_mats = glob.glob(os.path.join(day_path_mat, 'EEG/boneeggnd{:02d}-{:d}-{:02d}.mat'.\n",
    "                                                              format(key['day'], key['epoch_id']+1, key['tet_id']+1)))\n",
    "            if len(lfp_filepath_eeggnd_mats) > 1:\n",
    "                self.warn.duplicate(key, lfp_filepath_eeggnd_mats)\n",
    "\n",
    "            lfp_filepath_eeggnd_mat = lfp_filepath_eeggnd_mats[0]\n",
    "            \n",
    "            key['lfp_filepath_eeggnd_mat'] = lfp_filepath_eeggnd_mat\n",
    "        except IndexError:\n",
    "            self.warn.missing_field(key, 'eeggnd')\n",
    "        \n",
    "        self.insert1(key)\n",
    "\n",
    "\n",
    "@schema\n",
    "class RippleDetectionConfig(dj.Lookup):\n",
    "    definition = \"\"\"\n",
    "    rip_alg : varchar(20)\n",
    "    rip_detect_thresh : decimal(5,2)\n",
    "    rip_min_thresh_dur : decimal(6,4)\n",
    "    rip_tet_filter : varchar(100)\n",
    "    ---\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    contents = [['cons', 2.0, 0.0300, \"'(isequal($validripple, 1))'\"]]\n",
    "\n",
    "\n",
    "@schema\n",
    "class RippleInterval(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Epoch\n",
    "    -> RippleDetectionConfig\n",
    "    ---\n",
    "    part_path: varchar(200)\n",
    "    path_abs: varchar(200)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, suppress_print=False, arg=None):\n",
    "        self.warn = WarningTracker(suppress_print=suppress_print)\n",
    "        super().__init__(arg)\n",
    "\n",
    "    class LFPSource(dj.Part):\n",
    "        definition = \"\"\"\n",
    "        -> LFP\n",
    "        -> RippleInterval\n",
    "        ---\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    def make(self, key):\n",
    "        print(key)\n",
    "        anim_name_short, day_path_mat = (Animal() * (Day() & key)).fetch1('anim_name_short', 'day_path_mat')\n",
    "        anim_name = key['anim_name']\n",
    "        day = key['day']\n",
    "        epoch_id = key['epoch_id']\n",
    "        rip_alg = key['rip_alg']\n",
    "        rip_mat_fp = glob.glob(os.path.join(day_path_mat, '{:s}ripples{:s}{:02d}.mat'.\n",
    "                                            format(anim_name_short, rip_alg, key['day'])))\n",
    "        \n",
    "        if len(rip_mat_fp) > 1:\n",
    "            self.warn.duplicate_entries(key, rip_mat_fp[0], rip_mat_fp[1:], use_first=True)\n",
    "        elif len(rip_mat_fp) == 0:\n",
    "            self.warn.missing_entries(key, os.path.join(day_path_mat, '{:s}ripples{:s}{:02d}.mat'.\n",
    "                                      format(anim_name_short, rip_alg, key['day'])))\n",
    "        \n",
    "        try:\n",
    "            rip_mat = self.day_rip_mats[key['day']-1]\n",
    "        except AttributeError:\n",
    "            self.day_rip_mats = {}\n",
    "            rip_mat = loadmat(rip_mat_fp[0])[str('ripples{:s}'.format(rip_alg))][0]\n",
    "            self.day_rip_mats[key['day']-1] = rip_mat\n",
    "        except KeyError:\n",
    "            rip_mat = loadmat(rip_mat_fp[0])['ripples{:s}'.format(rip_alg)][0]\n",
    "            self.day_rip_mats[key['day']-1] = rip_mat\n",
    "        \n",
    "        rip_epoch = rip_mat[key['day']-1][0][key['epoch_id']][0]\n",
    "        f\n",
    "\n",
    "@schema\n",
    "class RawSpikes(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> TetrodeEpoch\n",
    "    ---\n",
    "    raw_spike_path: varchar(200)\n",
    "    \"\"\"\n",
    "\n",
    "@schema\n",
    "class Position(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Epoch\n",
    "    ---\n",
    "    pos_path: varchar(200)\n",
    "    \"\"\"\n",
    "\n",
    "@schema\n",
    "class LinearPosition(dj.Imported):\n",
    "    definition = \"\"\"\n",
    "    -> Position\n",
    "    ---\n",
    "    lin_pos_path: varchar(200)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anim = Animal(suppress_print=True)\n",
    "anim.insert1({'anim_name': 'Bond', 'anim_name_short': 'bon', 'anim_path_raw': '/opt/data/daliu/other/mkarlsso/bond/', \n",
    "              'anim_path_mat': '/opt/data/daliu/other/mkarlsso/Bon/'})\n",
    "display(anim)\n",
    "day = Day(suppress_print=True)\n",
    "day.populate()\n",
    "display(day)\n",
    "epoch = Epoch(suppress_print=True)\n",
    "epoch.populate()\n",
    "display(epoch)\n",
    "tet = Tetrode(suppress_print=True)\n",
    "tet.populate()\n",
    "display(tet)\n",
    "tet_ep = TetrodeEpoch(suppress_print=True)\n",
    "tet_ep.populate(reserve_jobs=True)\n",
    "display(tet_ep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp.drop()\n",
    "lfp_raw.drop()\n",
    "lfp_gnd.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lfp = LFP(suppress_print=True)\n",
    "lfp.populate()\n",
    "display(lfp)\n",
    "lfp_raw = LFPRaw(suppress_print=True)\n",
    "lfp_raw.populate()\n",
    "display(lfp_raw)\n",
    "lfp_gnd = LFPGnd(suppress_print=True)\n",
    "lfp_gnd.populate()\n",
    "display(lfp_gnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip = RippleInterval()\n",
    "rip_config = RippleDetectionConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip_config.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip.LFPSource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.ERD(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}