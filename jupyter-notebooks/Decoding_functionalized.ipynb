{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os \n",
    "path_main = '/home/anna/Src/spykshrk_realtime'\n",
    "os.chdir(path_main)  # navigate to main spykshrk parent directory \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import holoviews as hv\n",
    "import loren_frank_data_processing as lfdp\n",
    "from loren_frank_data_processing import Animal\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "import trodes2SS\n",
    "import scipy as sp\n",
    "import sungod_util\n",
    "from spykshrk.franklab.data_containers import RippleTimes, pos_col_format#FlatLinearPosition, SpikeFeatures, Posteriors, \\\n",
    "         #EncodeSettings, pos_col_format, SpikeObservation, RippleTimes, DayEpochEvent, DayEpochTimeSeries\n",
    "from spykshrk.franklab.pp_decoder.pp_clusterless import OfflinePPEncoder, OfflinePPDecoder\n",
    "from spykshrk.franklab.pp_decoder.visualization import DecodeVisualizer\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO : \n",
    "# 1. improve save options. make convert posterior to xarray more flexible - make params optional \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define parameters\n",
    "rat_name = 'remy'\n",
    "day = 20      #previously:{'remy':[20], 'gus':[28], 'bernard':[23], 'fievel':[19]}\n",
    "epoch = 2   # previously:{'remy':[4], 'gus':[2], 'bernard':[4], 'fievel':[2]} \n",
    "\n",
    "# define data source filepaths\n",
    "path_base = '/data2/mcoulter/'\n",
    "raw_directory = path_base + 'raw_data/' + rat_name + '/'\n",
    "linearization_path = path_base + 'maze_info/'\n",
    "day_ep = str(day) + '_' + str(epoch)\n",
    "\n",
    "#tetrodes_dictionary = {'remy': [4,6,9,10,11,12,13,14,15,17,19,20,21,22,23,24,25,26,28,29,30], # for a 45 min runtime on virga use tetrodes 4,9,11,13,15,19,21,23,25,28,30\n",
    "#                       'gus': [6,7,8,9,10,11,12,17,18,19,21,24,25,26,27,30], # list(range(6,13)) + list(range(17,22)) + list(range(24,28)) + [30]\n",
    "#                        'bernard': [1,2,3,4,5,7,8,10,11,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29], \n",
    "#                       'fievel': [1,2,3,5,6,7,8,9,10,11,12,14,15,16,17,18,19,20,22,23,24,25,27,28,29]}\n",
    "\n",
    "# if you want all ca1 tets with no deadchans, set tetlist to None. otherwise, specify list\n",
    "\n",
    "tetlist = None\n",
    "#tetlist = [4,6]\n",
    "\n",
    "if tetlist is None:\n",
    "    animalinfo  = {rat_name: Animal(directory=raw_directory, short_name=rat_name)}\n",
    "    tetinfo = lfdp.tetrodes.make_tetrode_dataframe(animalinfo)\n",
    "    tetinfo['ndtype'] = tetinfo['deadchans'].apply(lambda d: isinstance(d,np.ndarray)) # add column with datatype of deadchans entry\n",
    "    tmp = tetinfo['deadchans'][tetinfo['ndtype'].values].apply(lambda d: len(d))   # add length of deadchans list\n",
    "    tetinfo['ndlength'] = tmp   # store lengths as an additional column. no dead chans = length 0 \n",
    "    tetrodes = tetinfo.query('area==\"ca1\" & ndlength==0 & day==@day & epoch==@epoch').index.get_level_values('tetrode_number').unique().tolist()\n",
    "else:\n",
    "    tetrodes= tetlist\n",
    "    \n",
    "pos_bin_size = 5\n",
    "velocity_thresh_for_enc_dec = 4\n",
    "velocity_buffer = 0\n",
    "\n",
    "shift_amt_for_shuffle = 0\n",
    "\n",
    "discrete_tm_val=.99   # for classifier\n",
    "\n",
    "print(tetrodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# IMPORT and process data\n",
    "\n",
    "#initialize data importer\n",
    "datasrc = trodes2SS.TrodesImport(raw_directory, rat_name, [day], \n",
    "                       [epoch], tetrodes)\n",
    "# Import marks\n",
    "marks = datasrc.import_marks()\n",
    "print('original length: '+str(marks.shape[0]))\n",
    "# OPTIONAL: to reduce mark number, can filter by size. Current detection threshold is 100  \n",
    "marks = trodes2SS.threshold_marks(marks, maxthresh=2000,minthresh=100)\n",
    "# remove any big negative events (artifacts?)\n",
    "marks = trodes2SS.threshold_marks_negative(marks, negthresh=-999)\n",
    "print('after filtering: '+str(marks.shape[0]))\n",
    "\n",
    "# Import trials\n",
    "trials = datasrc.import_trials()\n",
    "\n",
    "# Import raw position\n",
    "linear_pos_raw = datasrc.import_pos(xy='x')   # this is basically just to pull in speed, will be replaced with linearized below\n",
    "#posY = datasrc.import_pos(xy='y')          #  OPTIONAL; useful for 2d visualization\n",
    "\n",
    "# if linearization exists, load it. if not, run the linearization.\n",
    "lin_output1 = linearization_path + rat_name + '/' + rat_name + '_' + day_ep + '_' + 'linearized_distance.npy'\n",
    "if os.path.exists(lin_output1) == False:\n",
    "    print('Linearization result doesnt exist. Doing linearization calculation')\n",
    "    sungod_util.run_linearization_routine(rat_name, day, epoch, linearization_path, raw_directory, gap_size=20)\n",
    "else: \n",
    "    print('Linearization found. Loading it')\n",
    "    lin_output2 = linearization_path + rat_name + '/' + rat_name + '_' + day_ep + '_' + 'linearized_track_segments.npy'\n",
    "    linear_pos_raw['linpos_flat'] = np.load(lin_output1)   #replace x pos with linerized \n",
    "    track_segment_ids = np.load(lin_output2)\n",
    "    \n",
    "# Import ripples\n",
    "rips_tmp = datasrc.import_rips(linear_pos_raw, velthresh=4) \n",
    "rips = RippleTimes.create_default(rips_tmp,1)  # cast to rippletimes obj\n",
    "print('Rips less than velocity thresh: '+str(len(rips)))\n",
    "# generate boundary definitions of each segment\n",
    "arm_coords, _ = sungod_util.define_segment_coordinates(linear_pos_raw, track_segment_ids)  # optional addition output of all occupied positions (not just bounds)\n",
    "\n",
    "#bin linear position \n",
    "binned_linear_pos, binned_arm_coords, pos_bins = sungod_util.bin_position_data(linear_pos_raw, arm_coords, pos_bin_size)\n",
    "\n",
    "# calculate bin coverage based on determined binned arm bounds   TO DO: prevent the annnoying \"copy of a slice\" error [prob need .values rather than a whole column]\n",
    "pos_bin_delta = sungod_util.define_pos_bin_delta(binned_arm_coords, pos_bins, linear_pos_raw, pos_bin_size)\n",
    "\n",
    "max_pos = binned_arm_coords[-1][-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide what to use as encoding and decoding data\n",
    "marks, binned_linear_pos = sungod_util.assign_enc_dec_set_by_velocity(binned_linear_pos, marks, velocity_thresh_for_enc_dec, velocity_buffer)\n",
    "\n",
    "# rearrange data by trials \n",
    "pos_reordered, marks_reordered, order = sungod_util.reorder_data_by_random_trial_order(trials, binned_linear_pos, marks)\n",
    "\n",
    "encoding_marks = marks_reordered.loc[marks_reordered['encoding_set']==1]\n",
    "decoding_marks = marks_reordered.loc[marks_reordered['encoding_set']==0]\n",
    "encoding_marks.drop(columns='encoding_set',inplace=True)  # drop these columns after use so they don't take up a bunch of extra space\n",
    "decoding_marks.drop(columns='encoding_set',inplace=True)\n",
    "\n",
    "print('Encoding spikes: '+str(len(encoding_marks)))\n",
    "print('Decoding spikes: '+str(len(decoding_marks)))\n",
    "\n",
    "encoding_pos = pos_reordered.loc[pos_reordered['encoding_set']==1]\n",
    "\n",
    "# apply shift for shuffling \n",
    "encoding_marks_shifted, shift_amount = sungod_util.shift_enc_marks_for_shuffle(encoding_marks, shift_amt_for_shuffle)\n",
    "# put marks back in chronological order for some reason\n",
    "encoding_marks_shifted.sort_index(level='time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate enc/dec settings. any parameter settable should be defined in parameter cell above and used here as a variable\n",
    "\n",
    "encode_settings = trodes2SS.AttrDict({'sampling_rate': 3e4,\n",
    "                                'pos_bins': np.arange(0,max_pos,1), # actually indices of valid bins. different from pos_bins above \n",
    "                                'pos_bin_edges': np.arange(0,max_pos + .1,1), # indices of valid bin edges\n",
    "                                'pos_bin_delta': pos_bin_delta, \n",
    "                                # 'pos_kernel': sp.stats.norm.pdf(arm_coords_wewant, arm_coords_wewant[-1]/2, 1),\n",
    "                                'pos_kernel': sp.stats.norm.pdf(np.arange(0,max_pos,1), max_pos/2, 1), #note that the pos_kernel mean should be half of the range of positions (ie 180/90)     \n",
    "                                'pos_kernel_std': 0, # 0 for histogram encoding model, 1+ for smoothing\n",
    "                                'mark_kernel_std': int(20), \n",
    "                                'pos_num_bins': max_pos, \n",
    "                                'pos_col_names': [pos_col_format(ii, max_pos) for ii in range(max_pos)], # or range(0,max_pos,10)\n",
    "                                'arm_coordinates': binned_arm_coords,   \n",
    "                                'spk_amp': 60,\n",
    "                                'vel': 0}) \n",
    "\n",
    "decode_settings = trodes2SS.AttrDict({'trans_smooth_std': 2,\n",
    "                                'trans_uniform_gain': 0.0001,\n",
    "                                'time_bin_size':60})\n",
    "\n",
    "sungod_trans_mat = sungod_util.calc_sungod_trans_mat(encode_settings, decode_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run encoder\n",
    "print('Starting encoder')\n",
    "\n",
    "encoder = OfflinePPEncoder(linflat=encoding_pos, dec_spk_amp=decoding_marks, encode_settings=encode_settings, \n",
    "                               decode_settings=decode_settings, enc_spk_amp=encoding_marks_shifted, dask_worker_memory=1e9,\n",
    "                               dask_chunksize = None)\n",
    "\n",
    "    #new output format from encoder: observ_obj\n",
    "observ_obj = encoder.run_encoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Starting decoder')\n",
    "\n",
    "decoder = OfflinePPDecoder(observ_obj=observ_obj, trans_mat=sungod_trans_mat, \n",
    "                               prob_no_spike=encoder.prob_no_spike,\n",
    "                               encode_settings=encode_settings, decode_settings=decode_settings, \n",
    "                               time_bin_size=decode_settings.time_bin_size, all_linear_position=binned_linear_pos)\n",
    "\n",
    "posteriors = decoder.run_decoder()\n",
    "print('Decoder finished!')\n",
    "print('Posteriors shape: '+ str(posteriors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY: save posteriors and position\n",
    "posterior_file_name = '/data2/mcoulter/remy/' + rat_name + '_' + str(day) + '_' + str(epoch) + '_shuffle_' + str(shift_amount) + '_posteriors_functionalized_TEST.nc'\n",
    "\n",
    "post1 = posteriors.apply_time_event(rips, event_mask_name='ripple_grp')\n",
    "post2 = post1.reset_index()\n",
    "post3 = trodes2SS.convert_dan_posterior_to_xarray(post2, tetrodes, \n",
    "                                        velocity_thresh_for_enc_dec, encode_settings, decode_settings, sungod_trans_mat, order, shift_amount)\n",
    "    #print(len(post3))\n",
    "post3.to_netcdf(posterior_file_name)\n",
    "print('Saved posteriors to '+posterior_file_name)\n",
    "\n",
    "    # to export linearized position to MatLab: again convert to xarray and then save as netcdf\n",
    "\n",
    "position_file_name = '/data2/mcoulter/remy/' + rat_name + '_' + str(day) + '_' + str(epoch) + '_shuffle_' + str(shift_amount) + '_linearposition_functionalized_TEST.nc'\n",
    "\n",
    "linearized_pos1 = binned_linear_pos.apply_time_event(rips, event_mask_name='ripple_grp')\n",
    "linearized_pos2 = linearized_pos1.reset_index()\n",
    "linearized_pos3 = linearized_pos2.to_xarray()\n",
    "linearized_pos3.to_netcdf(position_file_name)\n",
    "print('Saved linearized position to '+position_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run classifier \n",
    "sungod_no_offset = sungod_util.calc_sungod_trans_mat(encode_settings, decode_settings, uniform_gain=0)\n",
    "\n",
    "causal_state1, causal_state2, causal_state3, acausal_state1, acausal_state2, acausal_state3, trans_mat_dict = sungod_util.decode_with_classifier(decoder.likelihoods, \n",
    "                                                                                                                                 sungod_no_offset, \n",
    "                                                                                                                                 encoder.occupancy, discrete_tm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save classifier outputs \n",
    "base_name = '/data2/mcoulter/remy/' + rat_name + '_' + day_ep + '_shuffle_' + str(shift_amount) + '_posterior_'\n",
    "fname = 'causal'\n",
    "trodes2SS.convert_save_classifier(base_name, fname, causal_state1, causal_state2, causal_state3, tetrodes, decoder.likelihoods,\n",
    "                                  encode_settings, decode_settings, rips, velocity_thresh_for_enc_dec, velocity_buffer, sungod_no_offset, order, shift_amount)\n",
    "\n",
    "fname = 'acausal'\n",
    "trodes2SS.convert_save_classifier(base_name, fname, acausal_state1, acausal_state2, acausal_state3, tetrodes, decoder.likelihoods,\n",
    "                                  encode_settings, decode_settings, rips, velocity_thresh_for_enc_dec, velocity_buffer, sungod_no_offset, order, shift_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='bokeh' size=400 holomap='scrubber'\n",
    "%%opts RGB { +framewise} [height=100 width=250 aspect=2 colorbar=True]\n",
    "%%opts Points [height=100 width=250 aspect=2 ] (marker='o' color='#AAAAFF' size=1 alpha=0.7)\n",
    "%%opts Polygons (color='grey', alpha=0.5 fill_color='grey' fill_alpha=0.5)\n",
    "#%%opts Image {+framewise}\n",
    "\n",
    "# visualize posteriors - note will only work a small chunck of the posteriors table\n",
    "# currently this is ugly because posteriors, linpos, and rips all refer to different chunks of data\n",
    "\n",
    "dec_viz = DecodeVisualizer(posteriors[0:200000], linpos=binned_linear_pos.loc[(binned_linear_pos[\"linvel_flat\"]>4)], riptimes=rips[50:100], enc_settings=encode_settings)\n",
    "\n",
    "dec_viz.plot_all_dynamic(stream=hv.streams.RangeXY(), plt_range=100, slide=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
