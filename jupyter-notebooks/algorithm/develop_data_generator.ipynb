{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import holoviews as hv\n",
    "\n",
    "import json\n",
    "import functools\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "from spykshrk.franklab.pp_decoder.data_containers import FlatLinearPosition, SpikeFeatures, \\\n",
    "        EncodeSettings, pos_col_format, SpikeObservation\n",
    "from spykshrk.franklab.pp_decoder.util import normal_pdf_int_lookup\n",
    "from spykshrk.franklab.pp_decoder.pp_clusterless import OfflinePPDecoder\n",
    "from spykshrk.franklab.pp_decoder.visualization import DecodeVisualizer\n",
    "\n",
    "hv.extension('matplotlib')\n",
    "hv.extension('bokeh')\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cluster.close()\n",
    "    client.close()\n",
    "except:\n",
    "    print(\"No cluster or client\")\n",
    "    \n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=15)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnitGenerator:\n",
    "    def __init__(self, elec_grp, mark_mean, mark_cov, pos_mean, pos_var, peak_fr, sampling_rate):\n",
    "        self.elec_grp = elec_grp\n",
    "        self.mark_mean = mark_mean\n",
    "        self.mark_cov = mark_cov\n",
    "        self.pos_mean = pos_mean\n",
    "        self.pos_var = pos_var\n",
    "        self.rv_marks = sp.stats.multivariate_normal(mean=mark_mean, cov=np.diag(mark_cov))\n",
    "        self.rv_pos = sp.stats.norm(loc=pos_mean, scale=pos_var)\n",
    "        self.peak_fr = peak_fr\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "    def simulate_spikes_over_pos(self, linpos_flat):\n",
    "        prob_field = self.rv_pos.pdf(linpos_flat['linpos_flat'].values)/self.rv_pos.pdf(self.pos_mean)\n",
    "        \n",
    "        spike_train = sp.stats.bernoulli(p=self.peak_fr/self.sampling_rate * prob_field).rvs()\n",
    "        \n",
    "        \n",
    "        marks = np.atleast_2d(self.rv_marks.rvs(sum(spike_train))).astype('i4')\n",
    "        \n",
    "        sample_num = np.nonzero(spike_train)[0]\n",
    "\n",
    "        time_ind = linpos_flat.index[sample_num]\n",
    "        ind_levels = time_ind.levels.copy()\n",
    "        ind_levels.append([self.elec_grp])\n",
    "        ind_labels = time_ind.labels.copy()\n",
    "        ind_labels.append([0]*len(time_ind))\n",
    "        ind_names = time_ind.names.copy()\n",
    "        ind_names.append('elec_grp')\n",
    "        \n",
    "        new_ind = pd.MultiIndex(levels=ind_levels, labels=ind_labels, names=ind_names)\n",
    "        new_ind = new_ind.reorder_levels(['day', 'epoch', 'elec_grp', 'timestamp', 'time'])\n",
    "        #new_ind = new_ind.sortlevel(['day', 'epoch', 'elec_grp', 'timestamp', 'time'])\n",
    "\n",
    "        \n",
    "        spk_amp = SpikeFeatures(marks, columns=['c00', 'c01', 'c02', 'c03'],\n",
    "                                index=new_ind)\n",
    "                                                             \n",
    "        mark_linpos = linpos_flat.iloc[sample_num]\n",
    "        mark_linpos['elec_grp'] = self.elec_grp\n",
    "        mark_linpos.set_index('elec_grp', append=True, inplace=True)\n",
    "        mark_linpos = mark_linpos.reorder_levels(['day','epoch','elec_grp','timestamp','time'])\n",
    "        \n",
    "        return spk_amp, mark_linpos, prob_field\n",
    "    \n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encode_settings = AttrDict({'sampling_rate': 1000,\n",
    "                            'pos_bins': np.arange(0,100,1),\n",
    "                            'pos_bin_edges': np.arange(0,101,1),\n",
    "                            'pos_bin_delta': 1,\n",
    "                            'pos_kernel': sp.stats.norm.pdf(np.arange(0,100), 50, 10),\n",
    "                            'pos_kernel_std': 5, \n",
    "                            'mark_kernel_std': 20, \n",
    "                            'pos_num_bins': 100,\n",
    "                            'arm_coordinates': [[0,100]]})\n",
    "\n",
    "decode_settings = AttrDict({'trans_smooth_std': 5,\n",
    "                            'trans_uniform_gain': 0.001,\n",
    "                            'time_bin_size': 10})\n",
    "                            \n",
    "\n",
    "sampling_rate = 1000\n",
    "\n",
    "pos_time = np.arange(0,100000,1)\n",
    "pos_run = 50*np.cos(pos_time[0:17500]/(500*np.pi))+50\n",
    "pos_run = np.append(pos_run, ([pos_run[-1]]*45000))\n",
    "pos_run = np.append(pos_run, 50*np.cos(pos_time[62500:100000]/(500*np.pi))+50)\n",
    "pos_vel = np.concatenate([[0], np.diff(pos_run) * sampling_rate])\n",
    "\n",
    "linpos_flat_obj = FlatLinearPosition.from_numpy_single_epoch(1, 1, pos_time, pos_run, pos_vel, sampling_rate)\n",
    "\n",
    "mark_mean_range = [20,1000]\n",
    "mark_cov_range = [200,500]\n",
    "num_marks = 4\n",
    "num_units = 20\n",
    "firing_rate_range = [50,200]\n",
    "\n",
    "pos_range = [0, 100]\n",
    "pos_var_range = [10,20]\n",
    "\n",
    "mark_kernel = 40\n",
    "pos_kernel = 10\n",
    "\n",
    "unit_mean = np.random.randint(*mark_mean_range, [num_units, num_marks])\n",
    "unit_cov = np.random.randint(*mark_cov_range, [num_units, num_marks])\n",
    "unit_pos_mean = np.random.randint(*pos_range, [num_units])\n",
    "unit_pos_var = np.random.randint(*pos_var_range, [num_units])\n",
    "unit_fr = np.random.randint(*pos_var_range, [num_units])\n",
    "\n",
    "\n",
    "\n",
    "units = {}\n",
    "unit_spks = {}\n",
    "spk_amps = pd.DataFrame()\n",
    "for unit_ii in range(num_units):\n",
    "    units[unit_ii] = UnitGenerator(elec_grp=1,\n",
    "                                   mark_mean=unit_mean[unit_ii,:], mark_cov=unit_cov[unit_ii,:], \n",
    "                                   pos_mean=unit_pos_mean[unit_ii], pos_var=unit_pos_var[unit_ii], \n",
    "                                   peak_fr=unit_fr[unit_ii], sampling_rate=sampling_rate)\n",
    "\n",
    "    unit_marks, mark_pos, field = units[unit_ii].simulate_spikes_over_pos(linpos_flat_obj)\n",
    "    unit_spks[unit_ii] = unit_marks.merge(mark_pos, how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    spk_amps = spk_amps.append(unit_marks)\n",
    "spk_amps.sort_index(level='timestamp', inplace=True)\n",
    "\n",
    "spk_amps.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Curve(pos_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1,2,3,4],[2,2,2,2]]\n",
    "b = [[5,5,5,5],[6,6,6,6]]\n",
    "p = sp.stats.norm.pdf(np.expand_dims([0,1,2,3,4,5,6,7], 0),\n",
    "                      np.expand_dims(np.array([1,2]),1), \n",
    "                      1)\n",
    "\n",
    "c = normal_pdf_int_lookup(np.expand_dims(a,1), np.expand_dims(b,0), 1).prod(axis=2)\n",
    "\n",
    "print(c)\n",
    "r = np.matmul(c, p)\n",
    "print(r.shape)\n",
    "print(r)\n",
    "o = r/np.array([0.01,1,1,0.01,0.01,0.01,0.01,0.01])\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_observ_tet(dec_spk, enc_spk, tet_lin_pos, occupancy, encode_settings):\n",
    "    \n",
    "    pos_distrib_tet = sp.stats.norm.pdf(np.expand_dims(encode_settings.pos_bins, 0),\n",
    "                                        np.expand_dims(tet_lin_pos['linpos_flat'],1), \n",
    "                                        encode_settings.pos_kernel_std)\n",
    "\n",
    "    mark_contrib = normal_pdf_int_lookup(np.expand_dims(dec_spk, 1), \n",
    "                                         np.expand_dims(enc_spk,0), \n",
    "                                         encode_settings.mark_kernel_std)\n",
    "\n",
    "    all_contrib = np.prod(mark_contrib, axis=2)\n",
    "\n",
    "    observ = np.matmul(all_contrib, pos_distrib_tet)\n",
    "    \n",
    "    # occupancy normalize\n",
    "    observ = observ / occupancy\n",
    "    \n",
    "    # normalize each row\n",
    "    observ = observ / observ.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    ret_df = pd.DataFrame(observ, index=dec_spk.index, \n",
    "                          columns=[pos_col_format(pos_ii, observ.shape[1]) \n",
    "                                   for pos_ii in range(observ.shape[1])])\n",
    "    return ret_df\n",
    "\n",
    "\n",
    "occ, _ = np.histogram(a=linpos_flat_obj['linpos_flat'], bins=encode_settings.pos_bin_edges,normed=True)\n",
    "occ = np.convolve(occ, encode_settings.pos_kernel)[int(len(occ)/2):int(len(occ)*3/2)]\n",
    "\n",
    "grp = spk_amps.groupby('elec_grp')\n",
    "observations = {}\n",
    "task = []\n",
    "chunksize = 100\n",
    "for tet_id, spk_tet in grp:\n",
    "    spk_tet.index = spk_tet.index.droplevel('elec_grp')\n",
    "    tet_lin_pos = linpos_flat_obj.reindex(spk_tet.index)\n",
    "    \n",
    "    # Velocity threshold on spikes and position\n",
    "    #tet_lin_pos_thresh = tet_lin_pos.get_above_velocity(10.)\n",
    "    #spk_tet_thresh = spk_tet.reindex(tet_lin_pos_thresh.index)\n",
    "    tet_lin_pos_thresh = tet_lin_pos\n",
    "    spk_tet_thresh = spk_tet\n",
    "    \n",
    "    # Decode from all spikes\n",
    "    dask_spk_tet = dd.from_pandas(spk_tet.get_simple_index(), chunksize=chunksize)\n",
    "    \n",
    "    df_meta = pd.DataFrame([], columns=[pos_col_format(ii, encode_settings.pos_num_bins) \n",
    "                                        for ii in range(encode_settings.pos_num_bins)])\n",
    "    \n",
    "    # Setup decode of all spikes from encoding of velocity threshold spikes\n",
    "    task.append(dask_spk_tet.map_partitions(functools.partial(compute_observ_tet, enc_spk=spk_tet_thresh,\n",
    "                                                              tet_lin_pos=tet_lin_pos_thresh,\n",
    "                                                              occupancy=occ,\n",
    "                                                              encode_settings=encode_settings), \n",
    "                                            meta=df_meta))\n",
    "    \n",
    "results = dask.compute(*task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tet_ids = np.unique(spk_amps.index.get_level_values('elec_grp'))\n",
    "observ_tet_list = []\n",
    "grp = spk_amps.groupby('elec_grp')\n",
    "for tet_ii, (tet_id, grp_spk) in enumerate(grp):\n",
    "    tet_result = results[tet_ii]\n",
    "    tet_result.set_index(grp_spk.index, inplace=True)\n",
    "    observ_tet_list.append(tet_result)\n",
    "\n",
    "observ = pd.concat(observ_tet_list)\n",
    "observ_obj = SpikeObservation.from_df(observ.sort_index(level=['day', 'epoch', \n",
    "                                                               'timestamp', 'elec_grp']), )\n",
    "\n",
    "observ_obj['elec_grp'] = observ_obj.index.get_level_values('elec_grp')\n",
    "observ_obj.index = observ_obj.index.droplevel('elec_grp')\n",
    "\n",
    "observ_obj['position'] = (linpos_flat_obj['linpos_flat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "def plot_observ(ind):\n",
    "    print('more')\n",
    "    return hv.Overlay([hv.Curve(observ_obj.loc[:, 'x000':'x099'].iloc[ind+ii].values.T) for ii in range(5)])\n",
    "\n",
    "#Ind = Stream.define('stuff', ind=0)\n",
    "\n",
    "dmap = hv.DynamicMap(plot_observ, kdims=['ind'])\n",
    "dmap.redim.range(ind=(0, len(observ_obj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = dmap.redim.values(ind=[10])\n",
    "dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run PP decoding algorithm\n",
    "time_bin_size = 300\n",
    "\n",
    "decoder = OfflinePPDecoder(lin_obj=linpos_flat_obj, observ_obj=observ_obj,\n",
    "                           encode_settings=encode_settings, decode_settings=decode_settings, \n",
    "                           which_trans_mat='learned', time_bin_size=time_bin_size)\n",
    "\n",
    "posteriors = decoder.run_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot posteriors\n",
    "plt_ranges = [[0,100]]\n",
    "#plt_ranges = [[2461, 3405]]\n",
    "#plt_ranges = [[2930, 3000]]\n",
    "#plt_ranges = [[3295, 3325]]\n",
    "\n",
    "for plt_range in plt_ranges:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=[80,10])\n",
    "    DecodeVisualizer.plot_decode_image(posteriors, plt_range, encode_settings, x_tick=10)\n",
    "    print(ax)\n",
    "    DecodeVisualizer.plot_linear_pos(linpos_flat_obj, plt_range)\n",
    "    #DecodeVisualizer.plot_stim_lockout(ax, stim_lockout, plt_range, encode_settings.arm_coordinates[2][1] + 10)\n",
    "    \n",
    "    #plt.xlim(plt_range)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "#%%opts Scatter3D [fig_size=300 azimuth=75 elevation=20 yrotation=0]\n",
    "\n",
    "scatter = [hv.Scatter3D(mark_pos.loc[:,['linpos_flat','c01','c02']])\n",
    "           for elec_id, mark_pos in unit_spks.items()]\n",
    "\n",
    "\n",
    "overlay = hv.Overlay(scatter)\n",
    "\n",
    "overlay.opts({'Scatter3D':{'plot': {'fig_size':300, 'elevation':180}}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "%opts Scatter3D {+framewise}\n",
    "%opts Overlay {+framewise}\n",
    "\n",
    "from holoviews.streams import Stream, param\n",
    "\n",
    "\n",
    "def mark_plots(elevation, azimuth):\n",
    "    scatter = [hv.Scatter3D(mark_pos.loc[:,['linpos_flat','c01','c02']])\n",
    "               for elec_id, mark_pos in unit_spks.items()]\n",
    "    overlay = hv.Overlay(scatter)\n",
    "    overlay = overlay.opts({'Scatter3D':{'plot': {'fig_size':300, 'azimuth': azimuth, 'elevation':elevation},\n",
    "                                         'norm': {'framewise':True}}})\n",
    "    return overlay\n",
    "\n",
    "\n",
    "#holo = hv.HoloMap({(e,a): mark_plots(e,a) for e in range(0, 181, 20)\n",
    "#                   for a in range(-90,91,20)}, kdims=['e','a'])\n",
    "#holo\n",
    "\n",
    "dmap = hv.DynamicMap(callback=mark_plots, kdims=['elevation', 'azimuth'], cache_size=1)\n",
    "dmap = dmap.redim.range(elevation=(0,180), azimuth=(-90,90)).opts(norm=dict(framewise=True))\n",
    "dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "\n",
    "from ipywidgets import interact, interactive\n",
    "from bokeh.io import push_notebook, show\n",
    "from ipykernel.pylab.backend_inline import flush_figures\n",
    "\n",
    "overlay = overlay.opts({'Scatter3D':{'plot': {'fig_size':300, 'elevation':45, 'azimuth':45}}})\n",
    "\n",
    "renderer = hv.renderer('matplotlib')\n",
    "plot = (renderer.get_plot(overlay))\n",
    "\n",
    "def update(a, e):\n",
    "    %%output backend='matplotlib'\n",
    "\n",
    "    print(a, e)\n",
    "    sub = list(plot.subplots.values())[0]\n",
    "    sub.elevation = e\n",
    "    sub.update(0)\n",
    "    print(sub.elevation)\n",
    "    display(sub)\n",
    "    \n",
    "interactive_plot = interactive(update, a=(-180,180,1), e=(0,180,1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height='800px'\n",
    "interactive_plot"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}