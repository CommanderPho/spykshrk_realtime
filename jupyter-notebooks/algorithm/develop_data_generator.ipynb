{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/daliu/Src/spykshrk_realtime/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import json\n",
    "import functools\n",
    "import math\n",
    "\n",
    "# Modules for scientific computing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "# Modules for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import holoviews as hv\n",
    "\n",
    "# Modules for distributed computing\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "# Spykshrk modules for data analysis\n",
    "from spykshrk.franklab.data_containers import FlatLinearPosition, SpikeFeatures, \\\n",
    "        EncodeSettings, pos_col_format, SpikeObservation\n",
    "from spykshrk.franklab.pp_decoder.util import normal_pdf_int_lookup, gaussian\n",
    "from spykshrk.franklab.pp_decoder.pp_clusterless import OfflinePPEncoder, OfflinePPDecoder\n",
    "from spykshrk.franklab.pp_decoder.visualization import DecodeVisualizer\n",
    "\n",
    "# Visualization and display settings\n",
    "hv.extension('matplotlib')\n",
    "hv.extension('bokeh')\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup DASK workers using DASK distributed processes\n",
    "\n",
    "try:\n",
    "    cluster.close()\n",
    "    client.close()\n",
    "except:\n",
    "    print(\"No cluster or client running\")\n",
    "    \n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "#cluster = LocalCluster(n_workers=2, threads_per_worker=1, memory_limit=50e9, memory_target_fraction=0.01, \n",
    "#                       memory_pause_fraction=0.05)\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "min_worker_memory = np.inf\n",
    "for w in cluster.workers:\n",
    "    min_worker_memory = min(min_worker_memory, w.memory_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides DASK workers and forces the use of multiprocessing module\n",
    "\n",
    "# dask.config.set(scheduler = 'processes')\n",
    "# min_worker_memory = 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place cell/unit simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitGenerator:\n",
    "    \"\"\"\n",
    "    Neural activity simulator for tetrodes.  For each unit found on a single tetrode, models the unit as\n",
    "    a multivariate gaussians in 4-D amplitude (mark) space and firing rate in 1-D position space.  \n",
    "    Simulates spike trains as a poisson process with input of time and position.\n",
    "     Returns a table of spike times and their matching mark features.\n",
    "    \"\"\"\n",
    "    def __init__(self, elec_grp_id, mark_mean, mark_cov, pos_mean, pos_var, peak_fr, sampling_rate):\n",
    "        \"\"\"\n",
    "        Constructor to setup 4-D gaussian model of amplitude (mark) space and 1-D position.  The model is a \n",
    "        mark poisson distribution of when and what mark features spike events would have given an input of\n",
    "        position.\n",
    "        :param elec_grp_id: scalar ID for unique tetrodes\n",
    "        :param mark_mean: list specifying the mean of each unit's mark gaussian distribution (4-D)\n",
    "        :param mark_cov:  list specifying the covariance of each unit's mark gaussian distribution (4-D)\n",
    "        :param pos_mean: scalar specifying the mean of each unit's position gaussian distribution (1-D)\n",
    "        :param pos_var: scalar specifying the variance of each unit's position gaussian distribution (1-D)\n",
    "        :param peak_fr: scalar specifying the peak firing rate of each unit.\n",
    "        :param sampling_rate: uniform sampling rate to expect for simulation input\n",
    "        \"\"\"\n",
    "        self.elec_grp_id = elec_grp_id\n",
    "        self.mark_mean = mark_mean\n",
    "        self.mark_cov = mark_cov\n",
    "        self.pos_mean = pos_mean\n",
    "        self.pos_var = pos_var\n",
    "        self.rv_marks = sp.stats.multivariate_normal(mean=mark_mean, cov=np.diag(mark_cov))\n",
    "        self.rv_pos = sp.stats.norm(loc=pos_mean, scale=pos_var)\n",
    "        self.peak_fr = peak_fr\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "    def simulate_spikes_over_pos(self, linpos_flat):\n",
    "        \"\"\"Simulate spikes given a list of uniformly sampled position data.\n",
    "        \n",
    "        :param linpos_flat: a Pandas Dataframe of uniformly sampled position data. Index should be time\n",
    "                            and 'linpos_flat' should be the column name of 1-D positions\n",
    "        :return: a SpikeFeatures dataframe of simulated spike times and corresponding amplitude (mark) \n",
    "        features\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the probability of a spike occurring at each position depending on the \n",
    "        # firing rate - position map of the unit.\n",
    "        prob_field = self.rv_pos.pdf(linpos_flat['linpos_flat'].values)/self.rv_pos.pdf(self.pos_mean)\n",
    "        \n",
    "        # Simulates spike train by treating each time point as a bernoulli trial\n",
    "        spike_train = sp.stats.bernoulli(p=self.peak_fr/self.sampling_rate * prob_field).rvs()\n",
    "        \n",
    "        # Generate the mark features based on mark kernel.  Assumes mark probability is uniformly\n",
    "        # distributed over position.\n",
    "        marks = np.atleast_2d(self.rv_marks.rvs(sum(spike_train))).astype('i4')\n",
    "        \n",
    "        # list of spike indexes\n",
    "        sample_num = np.nonzero(spike_train)[0]\n",
    "\n",
    "        # reorganizing linpos data into a list of spike times\n",
    "        time_ind = linpos_flat.index[sample_num]\n",
    "        ind_levels = time_ind.levels.copy()\n",
    "        ind_levels.append([self.elec_grp_id])\n",
    "        ind_labels = time_ind.labels.copy()\n",
    "        ind_labels.append([0]*len(time_ind))\n",
    "        ind_names = time_ind.names.copy()\n",
    "        ind_names.append('elec_grp_id')\n",
    "        \n",
    "        # Organizes returning DataFrame \n",
    "        new_ind = pd.MultiIndex(levels=ind_levels, labels=ind_labels, names=ind_names)\n",
    "        new_ind = new_ind.reorder_levels(['day', 'epoch', 'elec_grp_id', 'timestamp', 'time'])\n",
    "        #new_ind = new_ind.sortlevel(['day', 'epoch', 'elec_grp', 'timestamp', 'time'])\n",
    "\n",
    "        # Packages Pandas data into a SpikeFeatures dataframe\n",
    "        spk_amp = SpikeFeatures(marks, columns=['c00', 'c01', 'c02', 'c03'],\n",
    "                                index=new_ind)\n",
    "        mark_linpos = linpos_flat.iloc[sample_num]\n",
    "        mark_linpos['elec_grp_id'] = self.elec_grp_id\n",
    "        mark_linpos.set_index('elec_grp_id', append=True, inplace=True)\n",
    "        mark_linpos = mark_linpos.reorder_levels(['day','epoch','elec_grp_id','timestamp','time'])\n",
    "        \n",
    "        return spk_amp, mark_linpos, prob_field\n",
    "    \n",
    "\n",
    "class AttrDict(dict):\n",
    "    \"\"\"\n",
    "    A helper class that takes a dictionary and maps all of it's keys as class attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration settings for place cell simulator and encoding/decoding algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Encoding and decoding settings for both simulator and algorithm\n",
    "encode_settings = AttrDict({'sampling_rate': 1000,\n",
    "                            'pos_bins': np.arange(0,100,1),\n",
    "                            'pos_bin_edges': np.arange(0,100.1,1),\n",
    "                            'pos_bin_delta': 1,\n",
    "                            'pos_kernel': sp.stats.norm.pdf(np.arange(0,100,1), 50, 1),\n",
    "                            'pos_kernel_std': 1, \n",
    "                            'mark_kernel_std': int(20), \n",
    "                            'pos_num_bins': 100,\n",
    "                            'pos_col_names': [pos_col_format(ii, 100) for ii in range(100)],\n",
    "                            'arm_coordinates': [[0,100]],\n",
    "                            'vel': 3,\n",
    "                            'spk_amp': 60})\n",
    "\n",
    "decode_settings = AttrDict({'trans_smooth_std': 5,\n",
    "                            'trans_uniform_gain': 0.001,\n",
    "                            'time_bin_size': 10})\n",
    "                            \n",
    "\n",
    "sampling_rate = 1000\n",
    "\n",
    "# Simulate Synthetic Position Data\n",
    "# Animal runs back and forth between two points, pausing for a few seconds in the middle.\n",
    "pos_time = np.arange(0,100*1000,1)\n",
    "pos_run = 50*np.cos(pos_time[0:17272]/(500*np.pi))+50\n",
    "pos_run = np.append(pos_run, ([pos_run[-1]]*39478))\n",
    "pos_run = np.append(pos_run, 50*np.cos(pos_time[56750:4000*1000]/(500*np.pi))+50)\n",
    "pos_vel = np.concatenate([[0], np.diff(pos_run) * sampling_rate])\n",
    "\n",
    "linpos_flat_obj = FlatLinearPosition.from_numpy_single_epoch(1, 1, pos_time, pos_run, pos_vel, sampling_rate, \n",
    "                                                             encode_settings.arm_coordinates)\n",
    "\n",
    "# Multiple Synthetic Unit Parameters\n",
    "mark_mean_range = [20,1000]\n",
    "mark_cov_range = [200,500]\n",
    "num_marks = 4\n",
    "num_units = 50\n",
    "firing_rate_range = [5,20]\n",
    "\n",
    "pos_field_range = [0, 100]\n",
    "pos_field_var_range = [5,10]\n",
    "\n",
    "unit_mean = np.random.randint(*mark_mean_range, [num_units, num_marks])\n",
    "unit_cov = np.random.randint(*mark_cov_range, [num_units, num_marks])\n",
    "\n",
    "#unit_pos_mean = np.random.randint(*pos_field_range, [num_units])\n",
    "#unit_pos_var = np.random.randint(*pos_field_var_range, [num_units])\n",
    "\n",
    "unit_pos_mean = np.linspace(*pos_field_range, num_units)\n",
    "unit_pos_var = np.array([pos_field_range[1]/num_units*2]*num_units)\n",
    "\n",
    "unit_fr = np.random.randint(*firing_rate_range, [num_units])\n",
    "\n",
    "\n",
    "# Loop that passes each unit's parameters to create a simulator per unit.  Assumes units\n",
    "# are all on same tetrode.  At the same time generate synthetic data using synthetic position data.\n",
    "units = {}\n",
    "unit_spks = {}\n",
    "spk_amps = pd.DataFrame()\n",
    "for unit_ii in range(num_units):\n",
    "    units[unit_ii] = UnitGenerator(elec_grp_id=1,\n",
    "                                   mark_mean=unit_mean[unit_ii,:], mark_cov=unit_cov[unit_ii,:], \n",
    "                                   pos_mean=unit_pos_mean[unit_ii], pos_var=unit_pos_var[unit_ii], \n",
    "                                   peak_fr=unit_fr[unit_ii], sampling_rate=sampling_rate)\n",
    "\n",
    "    unit_marks, mark_pos, field = units[unit_ii].simulate_spikes_over_pos(linpos_flat_obj)\n",
    "    unit_spks[unit_ii] = unit_marks.merge(mark_pos, how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "    spk_amps = spk_amps.append(unit_marks)\n",
    "spk_amps.sort_index(level='timestamp', inplace=True)\n",
    "\n",
    "# Merge all units, assuming all come from same tetrode.\n",
    "spk_amps = spk_amps[~spk_amps.index.duplicated(keep='first')]\n",
    "#spk_amps.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the synthetic data and inputs into the encoding/decoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_amps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output size=200 backend='matplotlib'\n",
    "%%opts Points [aspect=2] (marker='.')\n",
    "\n",
    "# Plot simulated position\n",
    "hv.Points(pos_run, kdims=[('samples', 'Samples'), ('pos', 'Position (cm)')], label='Synthetic position data for UnitGenerator and encoding algorithm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%output backend='matplotlib'\n",
    "%opts Scatter3D {+framewise}\n",
    "%opts Overlay {+framewise}\n",
    "\n",
    "from holoviews.streams import Stream, param\n",
    "\n",
    "# 3D interactive scatter plot of simulated tetrode\n",
    "def mark_plots(elevation, azimuth):\n",
    "    %%output backend='matplotlib'\n",
    "    scatter = [hv.Scatter3D(mark_pos.loc[:,['linpos_flat','c01','c02']])\n",
    "               for elec_id, mark_pos in unit_spks.items()]\n",
    "    overlay = hv.Overlay(scatter, label=\"Plot of spikes and their features in linpos_flat and amplitude channels c01 and c02\")\n",
    "    overlay = overlay.opts({'Scatter3D':{'plot': {'fig_size':400, 'azimuth': int(azimuth), \n",
    "                                                  'elevation': int(elevation)},\n",
    "                                         'norm': {'framewise':True}}})\n",
    "    return overlay\n",
    "\n",
    "\n",
    "#holo = hv.HoloMap({(e,a): mark_plots(e,a) for e in range(0, 181, 20)\n",
    "#                   for a in range(-90,91,20)}, kdims=['e','a'])\n",
    "#holo\n",
    "\n",
    "dmap = hv.DynamicMap(callback=mark_plots, kdims=['elevation', 'azimuth'], cache_size=1)\n",
    "dmap = dmap.redim.values(elevation=range(0,181,5),\n",
    "                         azimuth=range(-90,91,5)).opts(norm=dict(framewise=True))\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run encoding model and estimate position distribution for each encoding spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#%%prun -r -s cumulative\n",
    "\n",
    "# Setup encoding model and estimate the position distribution of each spike being encoded\n",
    "encoder = OfflinePPEncoder(linflat=linpos_flat_obj, enc_spk_amp=spk_amps, dec_spk_amp=spk_amps, \n",
    "                           #decode_settings=decode_settings,\n",
    "                           #encode_settings=encode_settings, dask_worker_memory=min_worker_memory)\n",
    "                           encode_settings=encode_settings, decode_settings=decode_settings,\n",
    "                           dask_chunksize=100)\n",
    "results = encoder.run_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Convert output of encoding model evaluation of each encoding spike to the SpikeObservation dataframe\n",
    "\n",
    "tet_ids = np.unique(spk_amps.index.get_level_values('elec_grp_id'))\n",
    "observ_tet_list = []\n",
    "grp = spk_amps.groupby('elec_grp_id')\n",
    "for tet_ii, (tet_id, grp_spk) in enumerate(grp):\n",
    "    tet_result = results[tet_ii]\n",
    "    tet_result.set_index(grp_spk.index, inplace=True)\n",
    "    observ_tet_list.append(tet_result)\n",
    "\n",
    "observ = pd.concat(observ_tet_list)\n",
    "observ_obj = SpikeObservation.create_default(observ.sort_index(level=['day', 'epoch', \n",
    "                                                               'timestamp', 'elec_grp_id']), \n",
    "                                             enc_settings=encode_settings)\n",
    "\n",
    "observ_obj['elec_grp_id'] = observ_obj.index.get_level_values('elec_grp_id')\n",
    "observ_obj.index = observ_obj.index.droplevel('elec_grp_id')\n",
    "\n",
    "observ_obj['position'] = (linpos_flat_obj['linpos_flat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%output backend='matplotlib' size=200\n",
    "%%opts Points (s=400 marker='x')\n",
    "\n",
    "# Setup plot to visualize estimated position distribution\n",
    "\n",
    "sel_distrib = observ_obj.loc[:, pos_col_format(0,encode_settings.pos_num_bins):         \n",
    "                                 pos_col_format(encode_settings.pos_num_bins-1,\n",
    "                                                encode_settings.pos_num_bins)]\n",
    "    \n",
    "sel_pos = observ_obj.loc[:, 'position']\n",
    "\n",
    "max = sel_distrib.max().max()\n",
    "    \n",
    "def plot_observ(ind):\n",
    "        \n",
    "    plot_list = []\n",
    "    for ii in range(5):\n",
    "        plot_list.append(hv.Curve(sel_distrib.iloc[ind+ii], extents=(0, 0, 100, max)))\n",
    "        plot_list.append(hv.Points((sel_pos.iloc[ind+ii], [0.005])))\n",
    "    return hv.Overlay(plot_list)\n",
    "        \n",
    "#Ind = Stream.define('stuff', ind=0)\n",
    "\n",
    "dmap = hv.DynamicMap(plot_observ, kdims=['ind'])\n",
    "dmap.redim.values(ind=list(range(0, len(observ_obj)-5, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run point process decoding algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run PP decoding algorithm\n",
    "time_bin_size = 10\n",
    "\n",
    "decoder = OfflinePPDecoder(observ_obj=observ_obj, trans_mat=encoder.trans_mat['learned'], \n",
    "                           prob_no_spike=encoder.prob_no_spike,\n",
    "                           encode_settings=encode_settings, decode_settings=decode_settings, \n",
    "                           time_bin_size=time_bin_size)\n",
    "\n",
    "posteriors = decoder.run_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = observ_obj.query('dec_bin == 2')\n",
    "l = [hv.Curve(row, label=str(ind), group=str(2)) for ind, row in q.get_distribution_view().iterrows()]\n",
    "hv.Overlay(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%output backend='bokeh' size=400 holomap='scrubber'\n",
    "%%opts RGB { +framewise} [height=100 width=150 colorbar=True]\n",
    "%%opts Points {+framewise} [height=100 width=150] (marker='o' size=4 alpha=0.5)\n",
    "%%opts Curve {+framewise} [height=100 width=100]\n",
    "\n",
    "dec_viz = DecodeVisualizer(posteriors, linpos=linpos_flat_obj, enc_settings=encode_settings)\n",
    "\n",
    "map = dec_viz.plot_all_dynamic(stream=hv.streams.RangeXY(), plt_range=100, slide=10)\n",
    "\n",
    "tapstream = hv.streams.Tap(source=map, x=0.16, y=0)\n",
    "\n",
    "def tap_print(x, y):\n",
    "    # get observ bin\n",
    "    observ_bin = math.floor((x - observ_obj.get_time_start())/(posteriors.dec_settings['time_bin_size'] / \n",
    "                                                               observ_obj.enc_settings['sampling_rate']))\n",
    "    sel_observ = observ_obj.query('dec_bin == @observ_bin')\n",
    "    sel_plots =  [hv.Curve(row, group=str(observ_bin)) \n",
    "                 for ii, (ind, row) in enumerate(sel_observ.get_distribution_view().iterrows())]\n",
    "    if len(sel_plots) == 0:\n",
    "        sel_plots = [hv.Curve([0,0],  group=str(observ_bin))]\n",
    "    sel_overlay = hv.Overlay(sel_plots)\n",
    "    return sel_overlay\n",
    "    \n",
    "map + hv.DynamicMap(tap_print, kdims=[], streams=[tapstream])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(posteriors)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test.loc[:, 'x000':'x099'].T)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}