{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys\n",
    "import os.path\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import text\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from spykshrk.realtime.datatypes import Datatypes\n",
    "import loren_frank_data_processing as lfdp\n",
    "from loren_frank_data_processing import Animal\n",
    "import trodes2SS\n",
    "import scipy as sp\n",
    "import sungod_util\n",
    "from spykshrk.franklab.data_containers import RippleTimes, pos_col_format#FlatLinearPosition, SpikeFeatures, Posteriors, \\\n",
    "         #EncodeSettings, pos_col_format, SpikeObservation, RippleTimes, DayEpochEvent, DayEpochTimeSeries\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to start up holoviews\n",
    "import holoviews as hv\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to test and make sure holoviews and bokeh are working\n",
    "coords = np.random.rand(50,2)\n",
    "points = hv.Points(coords)\n",
    "\n",
    "points.opts(color='k', marker='+', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell to read real-time hdf5 file\n",
    "\n",
    "#12-17, bill run session 3\n",
    "#hdf_file = '/stelmo/mcoulter/spykshrk_output/bill/20191217_bill_05_r3_westerlies_streaming.rec_merged.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new ripple numbering\n",
    "encoder_data3 = store['rec_3']\n",
    "decoder_data3 = store['rec_4']\n",
    "decoder_missed_spikes3 = store['rec_5']\n",
    "likelihood_data3 = store['rec_6']\n",
    "occupancy_data = store['rec_7']\n",
    "ripple_data3 = store['rec_1']\n",
    "#stim_state = store['rec_10']\n",
    "stim_lockout3 = store['rec_11']\n",
    "stim_message3 = store['rec_12']\n",
    "timing3 = store['rec_100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the most recent offline decode\n",
    "file_path = '/stelmo/mcoulter/oct_2019_decoding/remy_20_2_shuffle_0_posteriors_functionalized_150_decode_whole.h5'\n",
    "hdf_base = '/analysis'\n",
    "hdf_grps = 'decode/clusterless/offline/posterior'\n",
    "hdf_label = 'sungod_trans_mat'\n",
    "\n",
    "with pd.HDFStore(file_path, 'r') as store:\n",
    "    main_path = os.path.join(hdf_base, hdf_grps, hdf_label)\n",
    "    posteriors2 = store[main_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get linear position for offline\n",
    "#### Define parameters\n",
    "rat_name = 'remy'\n",
    "day =  20     #previously:{'remy':[20], 'gus':[28], 'bernard':[23], 'fievel':[19]}\n",
    "epoch = 2   # previously:{'remy':[4], 'gus':[2], 'bernard':[4], 'fievel':[2]} \n",
    "\n",
    "# define data source filepaths\n",
    "path_base = '/stelmo/mcoulter/'\n",
    "#path_base = '/mnt/vortex/mcoulter/'\n",
    "raw_directory = path_base + 'raw_data/' + rat_name + '/'\n",
    "linearization_path = path_base + 'maze_info/'\n",
    "day_ep = str(day) + '_' + str(epoch)\n",
    "\n",
    "#tetlist = None\n",
    "tetlist = [4]\n",
    "tetrodes= tetlist\n",
    "\n",
    "pos_bin_size = 5\n",
    "velocity_thresh_for_enc_dec = 4\n",
    "velocity_buffer = 0\n",
    "shift_amt_for_shuffle = 0\n",
    "use_enc_as_dec = 1   #if you want to use the encoding marks as the decoding marks\n",
    "discrete_tm_val=.99   # for classifier\n",
    "\n",
    "# IMPORT and process data\n",
    "#initialize data importer\n",
    "datasrc = trodes2SS.TrodesImport(raw_directory, rat_name, [day], \n",
    "                       [epoch], tetrodes)\n",
    "# Import trials\n",
    "trials = datasrc.import_trials()\n",
    "# Import raw position\n",
    "linear_pos_raw = datasrc.import_pos(xy='x')   # this is basically just to pull in speed, will be replaced with linearized below\n",
    "#posY = datasrc.import_pos(xy='y')          #  OPTIONAL; useful for 2d visualization\n",
    "\n",
    "# if linearization exists, load it. if not, run the linearization.\n",
    "lin_output1 = linearization_path + rat_name + '/' + rat_name + '_' + day_ep + '_' + 'linearized_distance.npy'\n",
    "lin_output2 = linearization_path + rat_name + '/' + rat_name + '_' + day_ep + '_' + 'linearized_track_segments.npy'\n",
    "linear_pos_raw['linpos_flat'] = np.load(lin_output1)   #replace x pos with linerized \n",
    "track_segment_ids = np.load(lin_output2)\n",
    "print('Linearization found. Loading it')\n",
    "\n",
    "#if os.path.exists(lin_output1) == False:\n",
    "#    print('Linearization result doesnt exist. Doing linearization calculation')\n",
    "#    sungod_util.run_linearization_routine(rat_name, day, epoch, linearization_path, raw_directory, gap_size=20)\n",
    "#    linear_pos_raw['linpos_flat'] = np.load(lin_output1)\n",
    "#    track_segment_ids = np.load(lin_output2)\n",
    "#else: \n",
    "\n",
    "# generate boundary definitions of each segment\n",
    "arm_coords, _ = sungod_util.define_segment_coordinates(linear_pos_raw, track_segment_ids)  # optional addition output of all occupied positions (not just bounds)\n",
    "\n",
    "#bin linear position \n",
    "binned_linear_pos, binned_arm_coords, pos_bins = sungod_util.bin_position_data(linear_pos_raw, arm_coords, pos_bin_size)\n",
    "\n",
    "# Import ripples\n",
    "rips_tmp = datasrc.import_rips(linear_pos_raw, velthresh=4) \n",
    "rips = RippleTimes.create_default(rips_tmp,1)  # cast to rippletimes obj\n",
    "print('Rips less than velocity thresh: '+str(len(rips)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of decoder_data to plot - loop through each ripple\n",
    "# how can we just generate the images for the mismatched ripples??\n",
    "\n",
    "realtime_posterior_sum_all = np.zeros((len(stim_lockout14[stim_lockout14['lockout_state']==1]),4))\n",
    "counter = -1\n",
    "\n",
    "for timestamp in stim_lockout14[stim_lockout14['lockout_state']==1]['timestamp'].values:\n",
    "    counter += 1\n",
    "    #print(counter)\n",
    "    if counter in non_matching['realtime_rip'].values:\n",
    "        print(counter)\n",
    "    #print(timestamp-30*300,timestamp+30*300)\n",
    "    #posterior from decode/stim message merged table\n",
    "        merged_to_plot = merged_decoder_lockout[(merged_decoder_lockout['bin_timestamp'] > timestamp-30*300) & \n",
    "                                            (merged_decoder_lockout['bin_timestamp'] < timestamp+30*300)]\n",
    "        merged_to_plot.set_index('bin_timestamp',inplace=True)\n",
    "\n",
    "        posterior_only_merged = merged_to_plot.drop(['rec_ind','wall_time','velocity','real_pos',\n",
    "                                                'spike_count','ripple','ripple_length','timestamp_shift',\n",
    "                                                'shortcut_message','box','arm1','arm2','arm3','arm4','arm5',\n",
    "                                                'arm6','arm7','arm8','posterior_max_arm','shortcut_message_sent',\n",
    "                                                'raw_x','raw_y','smooth_x','smooth_y','ripple_number_x',\n",
    "                                                'ripple_number_y','ripple_end','lockout_state','lockout_num',\n",
    "                                                'delay','next_bin'], axis=1)\n",
    "        # ripple time - generated from lockout_state\n",
    "        # lockout_state changes at posterior_lock end, so this should post_lock (now 50 msec)\n",
    "        ripples_to_plot = merged_to_plot.reset_index()\n",
    "        ripple_times_rt = ripples_to_plot.index[ripples_to_plot['lockout_state'] > 0].tolist()\n",
    "\n",
    "        # get timestamp when shortcut message was sent - try to just isolate single ripple\n",
    "        # bin_timestamp will show the delay - it will appear before the start of the ripple\n",
    "        # in contrast, lfp_timestamp would line up exactly with ripple start\n",
    "        shortcut_message_to_plot = stim_message14[(stim_message14['bin_timestamp'] > timestamp-30*30) & \n",
    "                                             (stim_message14['bin_timestamp'] < timestamp+30*100)]\n",
    "        merged_to_plot_index = merged_to_plot.reset_index()\n",
    "        # loop through multiple entries in shortcut_message\n",
    "        shortcut_message_times = np.zeros(shortcut_message_to_plot.shape[0])\n",
    "        for i in np.arange(0,shortcut_message_to_plot.shape[0]):\n",
    "            shortcut_message_times[i] = merged_to_plot_index.index[merged_to_plot_index['bin_timestamp'].values == shortcut_message_to_plot['bin_timestamp'][i:i+1].values].tolist()[0]\n",
    "        #shortcut_message_times = merged_to_plot_index.index[merged_to_plot_index['bin_timestamp'].values == shortcut_message_to_plot['bin_timestamp'].values].tolist()\n",
    "        shortcut_messages = shortcut_message_to_plot['shortcut_message_sent'].values*123\n",
    "    \n",
    "        #posterior_only\n",
    "        #print(posterior_only_merged.shape)\n",
    "    \n",
    "        #plot heatmap with posterior surrounding each ripple\n",
    "        post_heatmap = posterior_only_merged.transpose()\n",
    "        post_heatmap = post_heatmap.iloc[::-1]\n",
    "    \n",
    "        #plot title: include ripple number, max arm, and delay\n",
    "        title_index = int(len(merged_to_plot)*0.6)\n",
    "        max_arm = merged_to_plot[title_index:title_index+1]['posterior_max_arm'].values\n",
    "        #ripple_num = merged_to_plot[-2:-1]['ripple_number_y'].values\n",
    "        #f'Real-time posterior, ripple {max_arm}. Max arm {ripple_num}'\n",
    "        #ripple_num = merged_to_plot[ripple_num_index:ripple_num_index+1]['ripple_number_y'].values\n",
    "        ripple_num = merged_to_plot[title_index:title_index+1]['lockout_num'].values\n",
    "        message_delay = np.around(merged_to_plot[title_index:title_index+1]['delay'].values,decimals=0)\n",
    "\n",
    "        # fill in current row of posterior sum array - seems to work\n",
    "        realtime_posterior_sum_all[counter,0] = ripple_num\n",
    "        realtime_posterior_sum_all[counter,1] = max_arm\n",
    "        realtime_posterior_sum_all[counter,2] = shortcut_message_to_plot.shape[0]-1\n",
    "        realtime_posterior_sum_all[counter,3] = message_delay\n",
    "    \n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.title(f'Real-time posterior, Ripple {ripple_num} Max arm {max_arm} Delay {message_delay}')\n",
    "        ax = (sns.heatmap(post_heatmap,vmin=0, vmax=0.7))\n",
    "        #gap lines need to be inverse of where you would expect\n",
    "        ax.hlines([135-9,135-25,135-41,135-57,135-73,135-89,135-105,135-121], lw=2, color='w',*ax.get_xlim())\n",
    "        ax.scatter(np.arange(0,merged_to_plot.shape[0]),136-merged_to_plot['real_pos'].values,s=1,alpha=0.5,color='cyan')\n",
    "        #ax.scatter(stim_lockout['timestamp'].values,(stim_lockout['lockout_state'].values)*50,s=2,alpha=1)\n",
    "        ax.vlines(ripple_times_rt,lw=1,color='w',alpha=0.3,*ax.get_ylim())\n",
    "        # plot time when statescript message was sent, 100 = sent, 0 = not send\n",
    "        # if X is missing it means the timestamp didn't match perfectly above\n",
    "        # could take delay into account here: add delay/5\n",
    "        ax.scatter(shortcut_message_times+message_delay/5,124-shortcut_messages,s=40,color='yellow',marker='x')\n",
    "\n",
    "        # try to find ripple number\n",
    "        #ax.scatter(np.arange(0,merged_to_plot.shape[0]),136-merged_to_plot['ripple_number_y'].values+20,s=1,alpha=0.5,color='red')\n",
    "        # need to convert sent_message_list into values starting where beginning of this plot is x=0\n",
    "        #ax.vlines(100,lw=1,color='w',alpha=1,*ax.get_ylim())\n",
    "    \n",
    "        # final step: save the figure\n",
    "        #plt.savefig(f'/stelmo/mcoulter/posterior_plots_by_ripple/2-4-20/remy_20_2_realtime_2_5_nonmatching_{ripple_num}_timestamp_{timestamp}.jpg')\n",
    "\n",
    "# convert offline_posterior_sum_all array to pandas\n",
    "realtime_post_sum_summary = pd.DataFrame(data=realtime_posterior_sum_all,columns=('realtime_rip','rt_max_arm',\n",
    "                                                                                  'rt_two_messages','rt_delay'))\n",
    "realtime_post_sum_summary.shape\n",
    "# check numbering for ripples 4, 5, 6\n",
    "# also, looks like it never switch to an outer arm if the decode starts in the box\n",
    "# looks like many replays fail because it doesn't get 10 bins of data - this may be a result of empty bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to match offline, add 1958987 to realtime timestamps\n",
    "# shift for realtime ripple detection delay: subtract 2100\n",
    "#45317550 45321300\n",
    "\n",
    "realtime_rips = stim_lockout14[stim_lockout14['lockout_state']==1]\n",
    "offline_rips = rips.reset_index()\n",
    "offline_rips['adj_timestamp'] = offline_rips['timestamp']\n",
    "realtime_rips['adj_timestamp'] = realtime_rips['timestamp']+1958987-2100\n",
    "realtime_rips\n",
    "# merge real-time and offline ripples \n",
    "# offline ripple start is 50-100 msec before real-time\n",
    "# try a tolerance of 100 msec = 3000 timestamps\n",
    "merged_ripple_times = pd.merge_asof(offline_rips,realtime_rips,on='adj_timestamp',tolerance=3000,direction='nearest')\n",
    "matching_offline_rips = merged_ripple_times[merged_ripple_times['lockout_num']>0]\n",
    "matching_offline_rips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows number of offline ripples that match two realtime ripples (duplicates)\n",
    "# look at value of 0 bin in histogram (23)\n",
    "plt.hist(matching_offline_rips['lockout_num'].diff().values,bins=np.arange(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offline posteriors, 150 uV threshold\n",
    "#offline arm ends 8, 25, 42, 59, 77, 94, 111, 128, 145 \n",
    "arm_coords = [[0,8],[13,25],[30,42],[47,59],[65,77],[82,94],[99,111],[116,128],[133,145]]\n",
    "\n",
    "# now need to plot offline ripples that are mismatched\n",
    "\n",
    "posterior_offline = posteriors2.reset_index()\n",
    "offline_pos = binned_linear_pos.reset_index()\n",
    "merged_off_post_pos = pd.merge_asof(posterior_offline,offline_pos,on='timestamp',direction='nearest')\n",
    "offline_posterior_sum_all = np.zeros((len(matching_offline_rips),12))\n",
    "\n",
    "for index, rip_timestamp in enumerate(matching_offline_rips['timestamp_x']):\n",
    "    if matching_offline_rips['lockout_num'][index:index+1].values in non_matching['realtime_rip'].values:\n",
    "        print(matching_offline_rips['lockout_num'][index:index+1].values)\n",
    "        #print(rip_timestamp-30*300,rip_timestamp+30*300,index)\n",
    "\n",
    "        posterior_to_plot = merged_off_post_pos[(merged_off_post_pos['timestamp'] > rip_timestamp-30*300) & \n",
    "                                            (merged_off_post_pos['timestamp'] < rip_timestamp+30*300)]\n",
    "\n",
    "        realtime_ripple_num = matching_offline_rips['lockout_num'][index:index+1].values\n",
    "        ripple_num_index = int(len(posterior_to_plot)*0.55)\n",
    "        ripple_num = posterior_to_plot[ripple_num_index:ripple_num_index+1]['ripple_grp'].values\n",
    "    \n",
    "        # calculate posterior sum during ripple\n",
    "        # we need to only take out the time when ripple_grp matches ripple_grp at the middle of the plotting bin\n",
    "    \n",
    "        #post_sum_times = posterior_to_plot[posterior_to_plot['ripple_grp'] > 0]\n",
    "        post_sum_times = posterior_to_plot[posterior_to_plot['ripple_grp'] == ripple_num[0]]\n",
    "        ripple_length = post_sum_times.shape[0]\n",
    "        # sum each arm over whole ripple\n",
    "        post_sum_ripple = np.zeros((ripple_length,9))\n",
    "        for i in np.arange(0,ripple_length):\n",
    "            if i == 0:\n",
    "                for j in np.arange(0,len(arm_coords),1):\n",
    "                    post_sum_ripple[i,j] = post_sum_times.iloc[i,4:150].values[arm_coords[j][0]:arm_coords[j][1]].sum()\n",
    "\n",
    "            else:\n",
    "                for j in np.arange(0,len(arm_coords),1):\n",
    "                    post_sum_ripple[i,j] = post_sum_ripple[i-1,j] + post_sum_times.iloc[i,4:150].values[arm_coords[j][0]:arm_coords[j][1]].sum()\n",
    "            \n",
    "        # normalize sum of whole ripple - this is the final row\n",
    "        post_sum_ripple[i] = post_sum_ripple[i]/post_sum_ripple[i].sum()\n",
    "    \n",
    "        ripple_times = posterior_to_plot.index[posterior_to_plot['ripple_grp'] > 0].tolist()\n",
    "\n",
    "        posterior_offline1 = posterior_to_plot.drop(['day_x','epoch_x','timestamp','time_x','num_spikes','dec_bin',\n",
    "                                                 'ripple_grp','day_y','epoch_y','time_y','linpos_flat',\n",
    "                                                 'linvel_flat'], axis=1)\n",
    "\n",
    "        posterior_offline2 = posterior_offline1.fillna(0)\n",
    "        post_heatmap = posterior_offline2.transpose()\n",
    "        post_heatmap = post_heatmap.iloc[::-1]\n",
    "\n",
    "        posterior_sum = np.around(post_sum_ripple[-1:],decimals=1)\n",
    "        posterior_sum = posterior_sum[0]\n",
    "    \n",
    "        if len(np.argwhere(posterior_sum>=0.5)):\n",
    "            arm_max = np.argwhere(posterior_sum>=0.5)[0][0]\n",
    "        else:\n",
    "            arm_max = 99 \n",
    "\n",
    "        # fill in current row of posterior sum array - seems to work\n",
    "        offline_posterior_sum_all[index,0] = ripple_num\n",
    "        offline_posterior_sum_all[index,1] = realtime_ripple_num\n",
    "        offline_posterior_sum_all[index,2] = arm_max\n",
    "        offline_posterior_sum_all[index,3] = posterior_sum[0]\n",
    "        offline_posterior_sum_all[index,4] = posterior_sum[1]\n",
    "        offline_posterior_sum_all[index,5] = posterior_sum[2]\n",
    "        offline_posterior_sum_all[index,6] = posterior_sum[3]\n",
    "        offline_posterior_sum_all[index,7] = posterior_sum[4]\n",
    "        offline_posterior_sum_all[index,8] = posterior_sum[5]\n",
    "        offline_posterior_sum_all[index,9] = posterior_sum[6]\n",
    "        offline_posterior_sum_all[index,10] = posterior_sum[7]\n",
    "        offline_posterior_sum_all[index,11] = posterior_sum[8]\n",
    "                                     \n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.title(f'Off rip {ripple_num}. RT rip {realtime_ripple_num} Arm {arm_max} {posterior_sum}')\n",
    "        ax = (sns.heatmap(post_heatmap,vmin=0, vmax=0.7))\n",
    "        ax.hlines([145-10,145-27,145-44,145-61,145-79,145-96,145-113,145-130], lw=2, color='w',*ax.get_xlim())\n",
    "        ax.scatter(np.arange(0,posterior_to_plot.shape[0]),145-posterior_to_plot['linpos_flat'].values,s=2,alpha=1,color='cyan')\n",
    "        ax.vlines([ripple_times-posterior_to_plot.index[0]],lw=1,color='w',alpha=0.2,*ax.get_ylim())\n",
    "    \n",
    "        # final step: save the figure\n",
    "        #plt.savefig(f'/stelmo/mcoulter/posterior_plots_by_ripple/2-4-20/remy_20_2_offline_rip_nonmatching_{ripple_num}_real_rip_{realtime_ripple_num}_timestamp_{rip_timestamp}.jpg')\n",
    "\n",
    "# convert offline_posterior_sum_all array to pandas\n",
    "off_post_sum_summary = pd.DataFrame(data=offline_posterior_sum_all,columns=('offline_rip','realtime_rip',\n",
    "                                                                            'off_max_arm','box','arm1','arm2',\n",
    "                                                                            'arm3','arm4','arm5','arm6',\n",
    "                                                                            'arm7','arm8'))\n",
    "\n",
    "off_post_sum_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join offline and realtime replay summaries\n",
    "replay_summary_combined = pd.DataFrame.join(off_post_sum_summary,realtime_post_sum_summary,on='realtime_rip',\n",
    "                                            how='outer',lsuffix='off',rsuffix='rt')\n",
    "replay_combined_matching = replay_summary_combined[replay_summary_combined['offline_rip']>0]\n",
    "#replay_combined_matching = replay_combined_matching[replay_combined_matching['rt_two_messages']<1]\n",
    "#replay_summary_combined['realtime_ripoff'].values\n",
    "print(replay_combined_matching.shape)\n",
    "plt.hist(replay_combined_matching['realtime_rip'].diff(),bins=np.arange(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there a way to remove the duplicates? - should we?\n",
    "# include fractions in the print out summary\n",
    "\n",
    "# re-run without dealy\n",
    "# re-run with delay and full session\n",
    "\n",
    "# make histogram of classification for all replays\n",
    "\n",
    "#summarize matching between offline and realtime\n",
    "print('total matching:',replay_combined_matching.shape[0])\n",
    "\n",
    "#exact match\n",
    "print('exact match:',(replay_combined_matching[replay_combined_matching['off_max_arm'].values == replay_combined_matching['rt_max_arm'].values]).shape[0])\n",
    "\n",
    "# non-matching\n",
    "non_matching = replay_combined_matching[replay_combined_matching['off_max_arm'].values != replay_combined_matching['rt_max_arm'].values]\n",
    "print('non-matching:',non_matching.shape[0])\n",
    "# count for each arm in realtime replays\n",
    "print('realtime below 0.5:',non_matching[non_matching['rt_max_arm'] == 99].shape[0])\n",
    "print('realtime box:',non_matching[non_matching['rt_max_arm'] == 0].shape[0])\n",
    "print('realtime arm 1:',non_matching[non_matching['rt_max_arm'] == 1].shape[0])\n",
    "print('realtime arm 2:',non_matching[non_matching['rt_max_arm'] == 2].shape[0])\n",
    "print('realtime arm 3:',non_matching[non_matching['rt_max_arm'] == 3].shape[0])\n",
    "print('realtime arm 4:',non_matching[non_matching['rt_max_arm'] == 4].shape[0])\n",
    "print('realtime arm 5:',non_matching[non_matching['rt_max_arm'] == 5].shape[0])\n",
    "print('realtime arm 6:',non_matching[non_matching['rt_max_arm'] == 6].shape[0])\n",
    "print('realtime arm 7:',non_matching[non_matching['rt_max_arm'] == 7].shape[0])\n",
    "print('realtime arm 8:',non_matching[non_matching['rt_max_arm'] == 8].shape[0])\n",
    "\n",
    "# offline: no arm above 0.5\n",
    "print('offline < 0.5, mismatch total:',non_matching[non_matching['off_max_arm'] == 99].shape[0])\n",
    "print('offline < 0.5, realtime box:',non_matching[(non_matching['off_max_arm'] == 99) & (non_matching['rt_max_arm'] == 0)].shape[0])\n",
    "print('offline < 0.5, realtime other arm:',non_matching[(non_matching['off_max_arm'] == 99) & (non_matching['rt_max_arm'] != 0)].shape[0])\n",
    "\n",
    "# offline: box\n",
    "print('offline box, mismatch total:',non_matching[non_matching['off_max_arm'] == 0].shape[0])\n",
    "print('offline box, realtime < 0.5:',non_matching[(non_matching['off_max_arm'] == 0) & (non_matching['rt_max_arm'] == 99)].shape[0])\n",
    "print('offline box, realtime other arm:',non_matching[(non_matching['off_max_arm'] == 0) & (non_matching['rt_max_arm'] != 99)].shape[0])\n",
    "\n",
    "# offline: outer arm\n",
    "print('offline arm, mismatch total:',non_matching[(non_matching['off_max_arm'] > 0)&(non_matching['off_max_arm'] < 10)].shape[0])\n",
    "print('offline arm, realtime < 0.5:',non_matching[(non_matching['off_max_arm'] > 0)&(non_matching['off_max_arm'] < 10) & (non_matching['rt_max_arm'] == 99)].shape[0])\n",
    "print('offline arm, realtime box:',non_matching[(non_matching['off_max_arm'] > 0)&(non_matching['off_max_arm'] < 10) & (non_matching['rt_max_arm'] == 0)].shape[0])\n",
    "print('offline arm, realtime other arm:',non_matching[(non_matching['off_max_arm'] > 0)&(non_matching['off_max_arm'] < 10) & (non_matching['rt_max_arm'] > 0)&(non_matching['rt_max_arm'] < 10)].shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
